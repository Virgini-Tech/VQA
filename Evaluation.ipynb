{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhirajsrivastava\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:159: UserWarning: pylab import has clobbered these variables: ['shuffle']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import time\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.regularizers import l1,l2\n",
    "from tqdm import tqdm\n",
    "import heapq\n",
    "from sklearn.utils import shuffle\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "import bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_image_dict.pickle', 'rb') as handle:\n",
    "    all_image_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_image_dict.pickle', 'rb') as handle:\n",
    "    val_image_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get keys\n",
    "oldkeys = list(all_image_dict.keys())\n",
    "# Change \"item\" to \"object\"\n",
    "newkeys = [s.replace('Train', 'train2014/train2014') for s in oldkeys]\n",
    "# Get values\n",
    "vals = list(all_image_dict.values())\n",
    "# Create new dictionary by iterating over both newkeys and vals\n",
    "all_image_dict = {k: v for k, v in zip(newkeys, vals)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Get keys\n",
    "oldkeys = list(val_image_dict.keys())\n",
    "# Change \"item\" to \"object\"\n",
    "newkeys = [s.replace('Val', 'val2014/val2014') for s in oldkeys]\n",
    "# Get values\n",
    "vals = list(val_image_dict.values())\n",
    "# Create new dictionary by iterating over both newkeys and vals\n",
    "val_image_dict = {k: v for k, v in zip(newkeys, vals)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_dict.update(val_image_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = 'train2014/train2014'\n",
    "val_images = 'val2014/val2014'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_questions = 'v2_Questions_Train_mscoco/v2_OpenEnded_mscoco_train2014_questions.json'\n",
    "validation_questions = 'v2_Questions_Val_mscoco/v2_OpenEnded_mscoco_val2014_questions.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_questions, 'r') as f:\n",
    "    train_questions = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "with open(validation_questions, 'r') as f:\n",
    "    val_questions = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_annotations = 'v2_Annotations_Train_mscoco/v2_mscoco_train2014_annotations.json'\n",
    "validation_annotations = 'v2_Annotations_Val_mscoco/v2_mscoco_val2014_annotations.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_annotations, 'r') as f:\n",
    "    train_annotations = json.load(f)\n",
    "    \n",
    "    \n",
    "with open(validation_annotations, 'r') as f:\n",
    "    val_annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions_df = pd.DataFrame(train_questions['questions'])\n",
    "val_questions_df = pd.DataFrame(val_questions['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations_df = pd.DataFrame(train_annotations['annotations'])\n",
    "val_annotations_df = pd.DataFrame(val_annotations['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.merge(train_questions_df,train_annotations_df,  how='inner', left_on=['image_id','question_id'], right_on = ['image_id','question_id'])\n",
    "val_data = pd.merge(val_questions_df,val_annotations_df,  how='inner', left_on=['image_id','question_id'], right_on = ['image_id','question_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persons_answers(answers):\n",
    "    answers_dict = {}\n",
    "    score_dict = { 'yes' : 3, 'maybe' : 2, 'no' : 1 }\n",
    "    for _answer in answers:\n",
    "        score = score_dict[_answer['answer_confidence']]\n",
    "        if answers_dict.get(_answer['answer'],-1) != -1 :\n",
    "            answers_dict[_answer['answer']] += score\n",
    "        else:\n",
    "            answers_dict[_answer['answer']] = score\n",
    "\n",
    "    return sorted(list(answers_dict.items()),key = lambda x: x[1],reverse=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['derived_answer'] =  train_data[\"answers\"].apply(lambda x: persons_answers(x))\n",
    "val_data['derived_answer'] =  val_data[\"answers\"].apply(lambda x: persons_answers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data\n",
    "X_val = val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 448 \n",
    "img_height = 448\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dropna(inplace=True)\n",
    "X_val.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he's\": \"he is\",\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\"how's\": \"how is\",\"i'd\": \"i would\",\"i'll\": \"i will\",\"i'm\": \"i am\",\"i've\": \"i have\",\"isn't\": \"is not\",\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\"mustn't\": \"must not\",\"needn't\": \"need not\",\"oughtn't\": \"ought not\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\"that'd\": \"that would\",\"that's\": \"that is\",\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\"they'd\": \"they would\",\"they'll\": \"they will\",\"they're\": \"they are\",\"they've\": \"they have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what're\": \"what are\",\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\"where'd\": \"where did\",\"where's\": \"where is\",\"who'll\": \"who will\",\"who's\": \"who is\",\"won't\": \"will not\",\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\"you'll\": \"you will\",\"you're\": \"you are\"\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    '''Given a text this function removes the punctuations and returns the remaining text string'''\n",
    "    new_text = \"\"\n",
    "    text = text.lower()\n",
    "    i = 0\n",
    "    for word in text.split():\n",
    "        if i==0:\n",
    "            new_text = contractions.get(word,word)\n",
    "        else:\n",
    "            new_text = new_text + \" \" + contractions.get(word,word)\n",
    "        i += 1\n",
    "    return new_text.replace(\"'s\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['multiple_choice_answer'] = X_train['multiple_choice_answer'].apply(lambda x: preprocess_text(x))\n",
    "X_val['multiple_choice_answer'] = X_val['multiple_choice_answer'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = X_train['multiple_choice_answer'].values\n",
    "class_frequency = {}\n",
    "\n",
    "for _cls in all_classes:\n",
    "    if(class_frequency.get(_cls,-1)>0):\n",
    "        class_frequency[_cls] += 1\n",
    "    else:\n",
    "        class_frequency[_cls] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_tags = heapq.nlargest(1000, class_frequency, key = class_frequency.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the top 1000 classes\n",
    "X_train['multiple_choice_answer'] =  X_train['multiple_choice_answer'].apply(lambda x: x if x in common_tags else '')\n",
    "\n",
    "# removing question which has empty tags\n",
    "X_train = X_train[X_train['multiple_choice_answer'].apply(lambda x: len(x)>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clasess:  1000\n",
      "Shape of Answer Vectors in Train Data:  (388252, 1000)\n",
      "Shape of Answer Vectors in Validation Data:  (214354, 1000)\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelBinarizer()\n",
    "answer_vector_train = label_encoder.fit_transform(X_train['multiple_choice_answer'].apply(lambda x: x).values)\n",
    "answer_vector_val = label_encoder.transform(X_val['multiple_choice_answer'].apply(lambda x: x).values)\n",
    "\n",
    "ans_vocab = {l: i for i, l in enumerate(label_encoder.classes_)}\n",
    "\n",
    "print(\"Number of clasess: \", len(ans_vocab))\n",
    "print(\"Shape of Answer Vectors in Train Data: \", answer_vector_train.shape)\n",
    "print(\"Shape of Answer Vectors in Validation Data: \", answer_vector_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_question(text):\n",
    "    '''Given a text this function removes the punctuations and returns the remaining text string'''\n",
    "    new_text = \"<start>\"\n",
    "    text = text.lower()\n",
    "    for word in text.split():\n",
    "        new_text = new_text + \" \" + contractions.get(word, word)\n",
    "    new_text = new_text + \" <end>\"\n",
    "    return new_text.replace(\"'s\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-c9f73ee18296>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['question'] = X_train['question'].apply(lambda x: preprocess_question(x))\n"
     ]
    }
   ],
   "source": [
    "X_train['question'] = X_train['question'].apply(lambda x: preprocess_question(x))\n",
    "X_val['question'] = X_val['question'].apply(lambda x: preprocess_question(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in tokenizer: 12450\n",
      "Shape of Question Vectors in Train Data:  (388252, 24)\n",
      "Shape of Question Vectors in Validation Data:  (214354, 24)\n"
     ]
    }
   ],
   "source": [
    "#tokenization\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token = \"<unk>\", filters = '!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(X_train['question'].values)\n",
    "train_question_seqs = tokenizer.texts_to_sequences(X_train['question'].values)\n",
    "val_question_seqs = tokenizer.texts_to_sequences(X_val['question'].values)\n",
    "\n",
    "print(\"Number of words in tokenizer:\", len(tokenizer.word_index))\n",
    "ques_vocab = tokenizer.word_index\n",
    "\n",
    "#Padding\n",
    "#tokenizer.word_index['<pad>'] = 0\n",
    "#tokenizer.index_word[0] = '<pad>'\n",
    "question_vector_train = tf.keras.preprocessing.sequence.pad_sequences(train_question_seqs, padding='post')\n",
    "question_vector_val = tf.keras.preprocessing.sequence.pad_sequences(val_question_seqs,padding='post',maxlen=question_vector_train.shape[1])\n",
    "\n",
    "print(\"Shape of Question Vectors in Train Data: \", question_vector_train.shape)\n",
    "print(\"Shape of Question Vectors in Validation Data: \", question_vector_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_train = X_train['image_id'].apply(lambda x:  train_images + '/COCO_train2014_' + '%012d.jpg' % (x)).values\n",
    "image_paths_val = X_val['image_id'].apply(lambda x:  val_images + '/COCO_val2014_' + '%012d.jpg' % (x)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoattentionModel(tf.keras.layers.Layer):\n",
    "    def __init__(self):#, num_embeddings, num_classes, embed_dim=512, k=30\n",
    "        super().__init__()\n",
    "        self.num_classes = len(ans_vocab)\n",
    "        self.hidden_size = 512\n",
    "        self.dropout = 0.3\n",
    "        self.num_embeddings = len(ques_vocab)+1\n",
    "\n",
    "        self.image_dense = tf.keras.layers.Dense(self.hidden_size, kernel_initializer = tf.keras.initializers.glorot_normal(seed=15)) \n",
    "        self.image_corr = tf.keras.layers.Dense(self.hidden_size, kernel_initializer = tf.keras.initializers.glorot_normal(seed=29))\n",
    "\n",
    "        self.image_atten_dense = tf.keras.layers.Dense(self.hidden_size, kernel_initializer = tf.keras.initializers.glorot_uniform(seed=17)) \n",
    "        self.question_atten_dens = tf.keras.layers.Dense(self.hidden_size, kernel_initializer = tf.keras.initializers.glorot_uniform(seed=28))\n",
    "        self.question_atten_dropout = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.image_atten_dropout = tf.keras.layers.Dropout(self.dropout)\n",
    "\n",
    "        self.ques_atten = tf.keras.layers.Dense(1, kernel_initializer = tf.keras.initializers.glorot_uniform(seed=21))\n",
    "\n",
    "        self.img_atten = tf.keras.layers.Dense(1, kernel_initializer = tf.keras.initializers.glorot_uniform(seed=33))\n",
    "\n",
    "        self.embed = tf.keras.layers.Embedding(self.num_embeddings, self.hidden_size,\n",
    "                                               embeddings_initializer = tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=23))\n",
    "        \n",
    "        self.unigram_conv = tf.keras.layers.Conv1D(filters = self.hidden_size, kernel_size = 1, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed=41))\n",
    "        self.bigram_conv  = tf.keras.layers.Conv1D(filters = self.hidden_size, kernel_size = 2, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed=58), dilation_rate = 2)\n",
    "        self.trigram_conv = tf.keras.layers.Conv1D(filters = self.hidden_size, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed=89), dilation_rate = 2)\n",
    "        self.max_pool = tf.keras.layers.MaxPool2D((3,1))\n",
    "        self.phrase_dropout = tf.keras.layers.Dropout(self.dropout)\n",
    "        \n",
    "        self.lstm = tf.keras.layers.LSTM(units = 512 , return_sequences=True, dropout = self.dropout,\n",
    "                                         kernel_initializer = tf.keras.initializers.glorot_uniform(seed=26),\n",
    "                                         recurrent_initializer = tf.keras.initializers.orthogonal(seed=54))\n",
    "        \n",
    "        self.tanh = tf.keras.layers.Activation('tanh')\n",
    "        self.softmax = tf.keras.layers.Activation('softmax')\n",
    "        \n",
    "        self.W_w_dropout = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.W_p_dropout = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.W_s_dropout = tf.keras.layers.Dropout(self.dropout)\n",
    "\n",
    "        self.W_w = tf.keras.layers.Dense(units = self.hidden_size, kernel_initializer = tf.keras.initializers.glorot_uniform(seed=32), input_shape = (self.hidden_size,))\n",
    "        self.W_p = tf.keras.layers.Dense(units = self.hidden_size, kernel_initializer = tf.keras.initializers.glorot_uniform(seed=49), input_shape = (2 * self.hidden_size, ))\n",
    "        self.W_s = tf.keras.layers.Dense(units = self.hidden_size, kernel_initializer = tf.keras.initializers.glorot_uniform(seed=31), input_shape = (2 * self.hidden_size, ))\n",
    "        \n",
    "        self.fc1_Dense = tf.keras.layers.Dense(units = 2 * self.hidden_size, activation='relu',\n",
    "                                               kernel_initializer = tf.keras.initializers.he_normal(seed=84))\n",
    "        self.fc1_dropout = tf.keras.layers.Dropout(self.dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(units = self.num_classes, activation='softmax',kernel_initializer = tf.keras.initializers.glorot_uniform(seed=91), input_shape = (self.hidden_size,))\n",
    "        \n",
    "        return\n",
    "\n",
    "    def call(self, image, question):#Image: B x 196 x 512\n",
    "        \n",
    "        image = self.image_dense(image)\n",
    "        image = self.tanh(image)\n",
    "\n",
    "        words = self.embed(question)    # Words: B x L x 51\n",
    "\n",
    "        unigrams =  tf.expand_dims(self.tanh(self.unigram_conv(words)), 1) # B x L x 512\n",
    "        bigrams  =  tf.expand_dims(self.tanh(self.bigram_conv(words)), 1)  # B x L x 512\n",
    "        trigrams =  tf.expand_dims(self.tanh(self.trigram_conv(words)), 1) # B x L x 512\n",
    "\n",
    "        phrase = tf.squeeze(self.max_pool(tf.concat((unigrams, bigrams, trigrams), 1)), axis=1)  # B x L x 512\n",
    "        phrase = self.tanh(phrase)\n",
    "        phrase = self.phrase_dropout(phrase)\n",
    "  \n",
    "        hidden = None\n",
    "        sentence = self.lstm(phrase)        # B x L x 512   \n",
    "\n",
    "        print(words.shape, phrase.shape, sentence.shape)\n",
    "        v_word, q_word = self.co_attention(image, words)\n",
    "        v_phrase, q_phrase = self.co_attention(image, phrase)\n",
    "        v_sent, q_sent = self.co_attention(image, sentence)\n",
    "\n",
    "        h_w = self.tanh(self.W_w(self.W_w_dropout(q_word + v_word)))\n",
    "        h_p = self.tanh(self.W_p(self.W_p_dropout(tf.concat(((q_phrase + v_phrase), h_w), axis=1))))\n",
    "        h_s = self.tanh(self.W_s(self.W_s_dropout(tf.concat(((q_sent + v_sent), h_p), axis=1))))\n",
    "\n",
    "        fc1 = self.fc1_Dense(self.fc1_dropout(h_s))\n",
    "        logits = self.fc(fc1)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def co_attention(self, img_feat, ques_feat):  # V : B x 512 x 196, Q : B x L x 512\n",
    "        img_corr = self.image_corr(img_feat)\n",
    "\n",
    "        weight_matrix = tf.keras.backend.batch_dot(ques_feat, img_corr, axes = (2, 2))\n",
    "        weight_matrix = self.tanh(weight_matrix)\n",
    "\n",
    "        ques_embed = self.image_atten_dense(ques_feat)\n",
    "        img_embed = self.question_atten_dens(img_feat)\n",
    "\n",
    "        transform_img = tf.keras.backend.batch_dot(weight_matrix, img_embed)\n",
    "\n",
    "        ques_atten_sum = self.tanh(transform_img + ques_embed)\n",
    "        ques_atten_sum = self.question_atten_dropout(ques_atten_sum)\n",
    "        ques_atten = self.ques_atten(ques_atten_sum)\n",
    "        ques_atten =  tf.keras.layers.Reshape((ques_atten.shape[1],))(ques_atten)\n",
    "        ques_atten =  self.softmax(ques_atten)\n",
    "\n",
    "        transform_ques = tf.keras.backend.batch_dot(weight_matrix, ques_embed, axes = (1, 1))\n",
    "        img_atten_sum = self.tanh(transform_ques+img_embed)\n",
    "        img_atten_sum = self.image_atten_dropout(img_atten_sum)\n",
    "        img_atten = self.img_atten(img_atten_sum)\n",
    "        img_atten = tf.keras.layers.Reshape((img_atten.shape[1],))(img_atten)\n",
    "        img_atten = self.softmax(img_atten)\n",
    "\n",
    "        ques_atten = tf.keras.layers.Reshape(( 1, ques_atten.shape[1]))(ques_atten)\n",
    "        img_atten = tf.keras.layers.Reshape(( 1, img_atten.shape[1]))(img_atten)\n",
    "\n",
    "        ques_atten_feat = tf.keras.backend.batch_dot(ques_atten,ques_feat)\n",
    "        print(ques_atten_feat)\n",
    "        ques_atten_feat = tf.keras.layers.Reshape(( ques_atten_feat.shape[-1],))(ques_atten_feat)\n",
    "\n",
    "        img_atten_feat =  tf.keras.backend.batch_dot(img_atten, img_feat)\n",
    "        print(img_atten_feat)\n",
    "        img_atten_feat = tf.keras.layers.Reshape((img_atten_feat.shape[-1],))(img_atten_feat)\n",
    "\n",
    "        return img_atten_feat, ques_atten_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    if epoch < 10:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.0001 * tf.math.exp(0.1 * (10 - epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callBacksList():\n",
    "    \"\"\"\n",
    "    returns list of callback's\n",
    "    \"\"\"\n",
    "    filepath = \"Models\" + ModelName + \"/optimum.hdf5\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = filepath, monitor = 'val_accuracy', verbose = 1, save_best_only = True, mode = 'auto')\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy', patience = 2)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', patience = 4, verbose = 1)\n",
    "    scheduler_lr = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=0 )\n",
    "\n",
    "    #directory for tensorboard to save evnts\n",
    "    log_dir= \"Models\" + \"logs/fit/\" + ModelName + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "    #print(\"TensorBoard Folder for this Execution\",log_dir)#creating TensorBoard call back,this will write all events to given folder\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 0)\n",
    "\n",
    "    history = tf.keras.callbacks.History()\n",
    "    callbacks_list = [scheduler_lr, early_stop, history, tensorboard_callback, checkpoint]\n",
    "    return callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_CoattentionModel():\n",
    "    image_input1 = tf.keras.layers.Input(shape = (14,14, 512))\n",
    "    image_input = tf.keras.layers.Reshape((-1, 512))(image_input1)\n",
    "\n",
    "    question_input = tf.keras.layers.Input(shape=(question_vector_train.shape[1],))\n",
    "\n",
    "    output = CoattentionModel()(image_input, question_input)#num_embeddings = len(ques_vocab), num_classes = len(ans_vocab), embed_dim = 512\n",
    "\n",
    "    model = tf.keras.models.Model(inputs = [image_input1, question_input], outputs = output)\n",
    "\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 24, 512) (None, 24, 512) (None, 24, 512)\n",
      "Tensor(\"coattention_model/MatMul_3:0\", shape=(None, 1, 512), dtype=float32)\n",
      "Tensor(\"coattention_model/MatMul_4:0\", shape=(None, 1, 512), dtype=float32)\n",
      "Tensor(\"coattention_model/MatMul_8:0\", shape=(None, 1, 512), dtype=float32)\n",
      "Tensor(\"coattention_model/MatMul_9:0\", shape=(None, 1, 512), dtype=float32)\n",
      "Tensor(\"coattention_model/MatMul_13:0\", shape=(None, 1, 512), dtype=float32)\n",
      "Tensor(\"coattention_model/MatMul_14:0\", shape=(None, 1, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model_1 = Build_CoattentionModel()\n",
    "l2_alpha = 0.001\n",
    "ModelName = 'Coattention'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.load_weights('Models/coattention.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_type = X_val['answer_type'].values \n",
    "answers = X_val['answers'].values\n",
    "question_type = X_val['question_type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_dict = {  _qtype : { 'top_1' : 0,'top_2' : 0,'top_3' : 0 ,'top_5' : 0,'count' : 0 } for _qtype in np.unique(question_type) }\n",
    "actual_list = []\n",
    "pred_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imageTensor(img, ques, qtype, answer,atype,answers):\n",
    "    img_tensor = all_image_dict[img.decode('utf-8')]\n",
    "    img_tensor = np.reshape(img_tensor,(-1,img_tensor.shape[-1]))\n",
    "    return img_tensor, ques, qtype, answer, atype,answers\n",
    "\n",
    "def createDataset(image_paths,question_vector,question_type,answer_vector,answer_type,answers):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, question_vector.astype(np.float32), question_type,answer_vector,answer_type,answers))\n",
    "\n",
    "    # using map to load the numpy files in parallel\n",
    "    dataset = dataset.map(lambda img, ques, qtype, answer, atype,answers : tf.numpy_function(get_imageTensor, [img, ques, qtype, answer, atype,answers], \n",
    "                                                                                     [tf.float32, tf.float32, tf.string, tf.int64, tf.string,tf.string]),\n",
    "                                      num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "    # shuffling and batching\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-1a89d3eaa639>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_paths_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_vector_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswer_vector_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manswer_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manswers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-4acac6238be1>\u001b[0m in \u001b[0;36mcreateDataset\u001b[1;34m(image_paths, question_vector, question_type, answer_vector, answer_type, answers)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreateDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquestion_vector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquestion_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manswer_vector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manswer_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manswers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manswer_vector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manswer_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manswers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# using map to load the numpy files in parallel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    779\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m     \"\"\"\n\u001b[1;32m--> 781\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m   \u001b[1;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m   4659\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_files\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4660\u001b[0m     \u001b[1;34m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4661\u001b[1;33m     \u001b[0melement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4662\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4663\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    127\u001b[0m           \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m           normalized_components.append(\n\u001b[1;32m--> 129\u001b[1;33m               ops.convert_to_tensor(t, name=\"component_%d\" % i, dtype=dtype))\n\u001b[0m\u001b[0;32m    130\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpack_as\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\profiler\\trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1620\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1621\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1623\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[1;31m# Unused.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m   \"\"\"\n\u001b[1;32m--> 271\u001b[1;33m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[0;32m    272\u001b[0m                         allow_broadcast=True)\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    281\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf.constant\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m   \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m   \u001b[1;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m   \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "dataset_val = createDataset(image_paths_val, question_vector_val, question_type, answer_vector_val,answer_type,answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (batch, (img_tensor, question, qtype, answer,atype,answers)) in (dataset_val):\n",
    "    y_pred = model.predict([img_tensor,question])\n",
    "\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        _qtype = qtype[i].numpy().decode('utf-8')\n",
    "        _atype = atype[i].numpy().decode('utf-8')\n",
    "        error_dict[_qtype]['count'] += 1 \n",
    "        \n",
    "        for _k in [1]:#[1,2,3,5]:\n",
    "            if _atype == 'yes/no' and _k > 1:\n",
    "                top_k = tf.math.top_k(y_pred, k = 1 ).indices.numpy()[i]\n",
    "            else:\n",
    "                top_k = tf.math.top_k(y_pred, k = _k ).indices.numpy()[i]\n",
    "\n",
    "            pred_classes = np.zeros(y_pred.shape[1])\n",
    "            pred_classes[top_k] = 1\n",
    "            actual_list.append(answers[i])\n",
    "            pred_list.append(pred_classes)\n",
    "            if np.dot(pred_classes,answer[i]) > 0:\n",
    "                error_dict[_qtype]['top_' + str(_k)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_metric(predictions, truths):\n",
    "    total = 0\n",
    "    correct_val=0.0\n",
    "    for prediction, truth in tqdm(zip(predictions, truths)):\n",
    "        truth = ast.literal_eval(truth.numpy().decode('utf-8'))#json.loads(truth.numpy().decode('utf-8').replace(\"\\'\", \"\\\"\"))\n",
    "        prediction = label_encoder.inverse_transform(np.array([prediction]))[0]\n",
    "        temp_count=0\n",
    "        total +=1\n",
    "    for _truth in truth:\n",
    "        if prediction == _truth['answer']:\n",
    "            temp_count+=1\n",
    "    # accuracy = min((# humans that provided that answer/3) , 1)\n",
    "    if temp_count>2:\n",
    "        correct_val+=1\n",
    "    else:\n",
    "        correct_val+=float(temp_count)/3\n",
    "    return (correct_val/total)*100\n",
    "\n",
    "model_metric(pred_list,actual_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EA  = pd.DataFrame(error_dict).T\n",
    "df_EA[\"Question_Type\"] = df_EA.index\n",
    "df_EA['Accuracy'] = df_EA['top_1']/df_EA['count'] * 100\n",
    "\n",
    "df_EA = df_EA.sort_values('Accuracy',ascending=False)\n",
    "\n",
    "ax = df_EA[['Question_Type','Accuracy']].plot.bar(stacked=False, figsize=(18,5), color='orange')\n",
    "plt.title('Question Type vs Accuract on Test Data')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
